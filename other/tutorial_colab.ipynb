{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Basic Example\n\nThis tutorial demonstrates the core concepts of TableVault through a practical example: building a document processing pipeline with searchable embeddings.\n\nYou can find the Colab version of this tutorial at: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](TODO_COLAB_LINK)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Dependencies\n\nFirst, install TableVault."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TableVault\n",
    "!pip install tablevault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Setup ArangoDB\n\nInstall and run ArangoDB directly on the Colab VM with vector index support enabled.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install ArangoDB 3.12 with root password pre-configured via debconf\n!sudo apt-get update -q\n!sudo apt-get install -y curl gnupg\n!sudo rm -f /etc/apt/sources.list.d/arangodb.list\n!sudo rm -f /usr/share/keyrings/arangodb-3.12.gpg\n!curl -fsSL https://download.arangodb.com/arangodb312/DEBIAN/Release.key | sudo gpg --dearmor -o /usr/share/keyrings/arangodb-3.12.gpg\n!echo 'deb [signed-by=/usr/share/keyrings/arangodb-3.12.gpg] https://download.arangodb.com/arangodb312/DEBIAN/ /' | sudo tee /etc/apt/sources.list.d/arangodb.list\n!printf 'arangodb3 arangodb3/password password rootpassword\\narangodb3 arangodb3/password_again password rootpassword\\n' | sudo debconf-set-selections\n!sudo apt-get update -q\n!sudo DEBIAN_FRONTEND=noninteractive apt-get install -y arangodb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport time\n\n# Stop any existing ArangoDB started by package post-install scripts\nsubprocess.run([\"sudo\", \"pkill\", \"-f\", \"arangod\"], check=False)\ntime.sleep(1)\n\n# Ensure runtime/data directories exist and are writable by the arangodb user\nsubprocess.run([\"sudo\", \"mkdir\", \"-p\",\n    \"/var/log/arangodb3\", \"/var/lib/arangodb3\", \"/var/lib/arangodb3-apps\", \"/var/run/arangodb3\"],\n    check=True)\nsubprocess.run([\"sudo\", \"chown\", \"-R\", \"arangodb:arangodb\",\n    \"/var/log/arangodb3\", \"/var/lib/arangodb3\", \"/var/lib/arangodb3-apps\", \"/var/run/arangodb3\"],\n    check=True)\n\n# Make sure the experimental vector index flag is present exactly once\nconfig_path = \"/etc/arangodb3/arangod.conf\"\nsubprocess.run([\"sudo\", \"sed\", \"-i\",\n    \"/^[[:space:]]*experimental-vector-index[[:space:]]*=.*/d\", config_path],\n    check=True)\nsubprocess.run([\"sudo\", \"sed\", \"-i\",\n    \"/^[[]server[]]/a experimental-vector-index = true\", config_path],\n    check=True)\n\n# Start arangod in the background as the arangodb user\nlog_path = \"/tmp/arangod.log\"\nlog_file = open(log_path, \"w\")\nproc = subprocess.Popen(\n    [\"sudo\", \"-u\", \"arangodb\", \"arangod\", \"--config\", config_path],\n    stdout=log_file, stderr=log_file,\n)\n\ntime.sleep(2)\nif proc.poll() is not None:\n    log_file.close()\n    raise RuntimeError(\n        f\"arangod exited early with code {proc.returncode}. Check {log_path} for details.\"\n    )\n\nprint(\"arangod started.\")\nprint(f\"Logs: {log_path}\")"
  },
  {
   "cell_type": "markdown",
   "source": "Verify ArangoDB is running and vector index creation works:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom arango import ArangoClient\n\nclient = ArangoClient(hosts=\"http://localhost:8529\")\n\nfor attempt in range(60):\n    try:\n        sys_db = client.db(\"_system\", username=\"root\", password=\"rootpassword\")\n        info = sys_db.version()\n        version = info.get(\"version\") if isinstance(info, dict) else info\n        print(f\"ArangoDB is ready: {version}\")\n        break\n    except Exception:\n        if attempt == 59:\n            raise RuntimeError(\"ArangoDB did not become ready in time. Check /tmp/arangod.log.\")\n        time.sleep(1)\n        print(f\"Waiting for ArangoDB... ({attempt + 1}/60)\")\n\n# Explicitly validate vector-index capability\ntest_col = \"__tv_colab_vector_check\"\nif sys_db.has_collection(test_col):\n    sys_db.delete_collection(test_col)\ncol = sys_db.create_collection(test_col)\ntry:\n    col.add_index({\n        \"type\": \"vector\",\n        \"name\": \"vec_idx\",\n        \"fields\": [\"embedding_4\"],\n        \"params\": {\"metric\": \"cosine\", \"dimension\": 4, \"nLists\": 1},\n    })\n    print(\"Vector index creation succeeded.\")\nfinally:\n    sys_db.delete_collection(test_col)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Initialize the Vault\n\nCreate a TableVault instance connected to your ArangoDB. The `process_name` identifies this run \u2014 all data written through this vault is attributed to the `document_pipeline` process, making it easy to trace where each item came from later."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tablevault import Vault\n",
    "\n",
    "# Create a new TableVault process\n",
    "vault = Vault(\n",
    "    user_id=\"tutorial_user\",\n",
    "    process_name=\"document_pipeline\",\n",
    "    arango_url=\"http://localhost:8529\",\n",
    "    arango_db=\"tutorial_db\",\n",
    "    new_arango_db=True,  # Start fresh\n",
    "    arango_root_password=\"rootpassword\"\n",
    ")\n",
    "\n",
    "print(\"Vault initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "TableVault organizes data into typed lists:\n\n- **Document lists**: Store text content\n- **Embedding lists**: Store vector embeddings\n- **Record lists**: Store structured metadata\n\nEach list stores items at sequential integer positions. Items across lists can be linked by position range to track lineage \u2014 for example, recording that embedding position 2 was derived from document positions 2\u20133."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document list for storing text chunks\n",
    "vault.create_document_list(\"research_papers\")\n",
    "\n",
    "# Create an embedding list (using 384-dim for this example)\n",
    "EMBEDDING_DIM = 384\n",
    "vault.create_embedding_list(\"paper_embeddings\", ndim=EMBEDDING_DIM)\n",
    "\n",
    "# Create a record list for metadata\n",
    "vault.create_record_list(\"paper_metadata\", column_names=[\"title\", \"author\", \"chunk_id\"])\n",
    "\n",
    "print(\"Item lists created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We'll add sample documents and their embeddings, tracking the lineage between them. The `input_items` argument on `append_embedding` records which source positions the embedding was derived from, forming an explicit link that can be queried later."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document chunks\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks are inspired by biological neurons.\",\n",
    "    \"Deep learning has revolutionized computer vision.\",\n",
    "    \"Transformers have changed natural language processing.\",\n",
    "]\n",
    "\n",
    "# Mock embedding function (replace with your actual model like sentence-transformers)\n",
    "def get_embedding(text):\n",
    "    import hashlib\n",
    "    import random\n",
    "\n",
    "    seed = int.from_bytes(hashlib.sha256(text.encode()).digest(), \"big\")\n",
    "    rng = random.Random(seed)\n",
    "    return [rng.random() for _ in range(EMBEDDING_DIM)]\n",
    "\n",
    "print(f\"Mock embedding dimension: {len(get_embedding('test'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add documents and their embeddings with lineage tracking\nfor idx, doc in enumerate(documents):\n    # Add document\n    vault.append_document(\"research_papers\", doc)\n\n    # Generate and add embedding with lineage tracking\n    embedding = get_embedding(doc)\n    vault.append_embedding(\n        \"paper_embeddings\",\n        embedding,\n        input_items={\"research_papers\": [idx, idx + 1]},  # Links to source document\n        index_rebuild_count=max(0, len(documents) - 1),  # Force index build for small demo sets\n    )\n\n    # Add metadata\n    vault.append_record(\"paper_metadata\", {\n        \"title\": f\"Paper Section {idx + 1}\",\n        \"author\": \"Tutorial Author\",\n        \"chunk_id\": idx\n    })\n\n    print(f\"Added document {idx + 1}: {doc[:50]}...\")\n\n    # All writes for this item are complete \u2014 safe to stop or pause the process here\n    vault.checkpoint_execution()\n\nhas_index = vault.has_vector_index(EMBEDDING_DIM)\nprint(f\"\\nVector index created: {has_index}\")\nif not has_index:\n    print(\"Vector index was not created; approximate search may be unavailable on this ArangoDB setup.\")\nprint(\"All documents added with lineage tracking!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`vault.checkpoint_execution()` marks the end of each loop iteration as a safe point for stopping or pausing. TableVault uses these markers to coordinate with other processes: if an external request to pause or stop this pipeline has been issued, it will only take effect at a checkpoint \u2014 never in the middle of a write or a pending API call. This also works the other direction: a paused process will only resume at a checkpoint, keeping the pipeline state consistent. Without checkpoints, a stop or pause request would be deferred indefinitely; with them, the pipeline can be interrupted or resumed cleanly between items."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Each item list can have a description \u2014 a short text and optional embedding that annotates what the list contains. Descriptions serve two purposes: they make lists self-documenting, and they act as a semantic filter when querying. In Step 9 you will see how `description_text` narrows a search to only the lists relevant to your query."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add semantic descriptions for queryability\n",
    "vault.create_description(\n",
    "    \"research_papers\",\n",
    "    description=\"Collection of machine learning research paper excerpts\",\n",
    "    embedding=get_embedding(\"machine learning research papers\")\n",
    ")\n",
    "\n",
    "vault.create_description(\n",
    "    \"paper_embeddings\",\n",
    "    description=\"Vector embeddings of research paper text chunks\",\n",
    "    embedding=get_embedding(\"document embeddings vectors\")\n",
    ")\n",
    "\n",
    "print(\"Descriptions added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Query Content\n",
    "\n",
    "Now let's query the stored content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents\n",
    "all_docs = vault.query_item_content(\"research_papers\")\n",
    "print(\"All documents:\")\n",
    "for i, doc in enumerate(all_docs):\n",
    "    print(f\"  [{i}]: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific document by index\n",
    "first_doc = vault.query_item_content(\"research_papers\", index=0)\n",
    "print(f\"First document: {first_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get item metadata\n",
    "metadata = vault.query_item_list(\"research_papers\")\n",
    "print(f\"Document list info: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lineage lets you trace exactly which source data produced each derived item. This is useful for debugging data quality issues, reproducing results, and auditing how your pipeline transformed data over time. You can traverse in either direction \u2014 from a derived item back to its sources, or from a source forward to everything derived from it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find what the embeddings were derived from\n",
    "parents = vault.query_item_parent(\"paper_embeddings\")\n",
    "print(f\"Embedding parents: {parents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find what was derived from the documents\n",
    "children = vault.query_item_child(\"research_papers\")\n",
    "print(f\"Document children: {children}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific range lineage\n",
    "first_embedding_source = vault.query_item_parent(\n",
    "    \"paper_embeddings\",\n",
    "    start_position=0,\n",
    "    end_position=1\n",
    ")\n",
    "print(f\"First embedding came from: {first_embedding_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "TableVault supports both vector similarity search over embeddings and full-text search over documents. Both query types accept optional filters to narrow the search scope: `description_text` restricts results to lists whose description matches, and `code_text` restricts to lists created by processes whose source code contains the given string."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search by embedding similarity\nquery_text = \"artificial intelligence and deep learning\"\nquery_embedding = get_embedding(query_text)\n\n# Find similar embeddings\nsimilar = vault.query_embedding_list(\n    embedding=query_embedding,\n    use_approx=False,\n)\nprint(f\"Similar embeddings: {similar}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search documents by text\n",
    "results = vault.query_document_list(\n",
    "    document_text=\"neural networks\"\n",
    ")\n",
    "print(f\"Documents matching 'neural networks': {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Combining these filters is especially useful in large vaults with many lists: you can target exactly the data that is semantically relevant and was produced by the right pipeline stage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter embedding search by description text\n# Only searches within lists whose description contains \"document embeddings\"\nsimilar_filtered = vault.query_embedding_list(\n    embedding=query_embedding,\n    description_text=\"document embeddings\",\n    use_approx=False,\n)\nprint(f\"Embeddings filtered by description: {similar_filtered}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter document search by description text and code text\n# description_text: restricts to lists whose description mentions \"research paper\"\n# code_text: restricts to lists created by processes whose code contains \"append_document\"\nresults_filtered = vault.query_document_list(\n    document_text=\"neural networks\",\n    description_text=\"research paper\",\n    code_text=\"append_document\",\n)\nprint(f\"Documents filtered by description and code: {results_filtered}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Every item in TableVault is attributed to the process that created it. Process queries let you audit the full output of a given pipeline run, or find which process was responsible for a particular item \u2014 useful when you have multiple pipelines writing to the same vault."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which process created these items\n",
    "creation_process = vault.query_item_creation_process(\"research_papers\")\n",
    "print(f\"Created by process: {creation_process}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Find all items created in this process\nprocess_items = vault.query_process_item(\"document_pipeline\")\nprint(f\"Items in process: {process_items}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
